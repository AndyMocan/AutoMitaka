所有节点：
yum install net-tools -y



1、拷贝所有的RPM包到controller01上，并在controller01上配置httpd服务

rpm -ivh mailcap
rpm -ivh httpd-tools
rpm -ivh httpd

systemctl start httpd

在controller01上把RPM包拷贝到/var/www/html/openstack 目录下
cp -r /opt/openstack/ /var/www/html/

# 在controller01上配置yum源
echo "
[base]
name=CentOS-Base
baseurl=http://192.168.1.219/openstack/base
enabled=1
gpgcheck=0
[mitaka]
name=mitaka
baseurl=http://192.168.1.219/openstack/openstack-mitaka
enabled=1
gpgcheck=0
[epel]
name=epel
baseurl=http://192.168.1.219/openstack/epel
enabled=1
gpgcheck=0
" > /etc/yum.repos.d/openstack-mitaka.repo


2、脚本所在的管理机器上执行如下操作
echo "
[base]
name=CentOS-Base
baseurl=http://192.168.1.219/openstack/base
enabled=1
gpgcheck=0
[mitaka]
name=mitaka
baseurl=http://192.168.1.219/openstack/openstack-mitaka
enabled=1
gpgcheck=0
[epel]
name=epel
baseurl=http://192.168.1.219/openstack/epel
enabled=1
gpgcheck=0
" > /etc/yum.repos.d/openstack-mitaka.repo

yum install python-pip -y
pip install --upgrade pip
pip install pariako

cd AutoMitak/conf/
配置settings.py
# coding:utf-8

"""配置文件"""

host_data = {
    "controller_node":{    # 控制节点和网络节点
        "controller":{"IP":"192.168.1.242", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
        "controller01":{"IP":"192.168.1.219", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
        "controller02":{"IP":"192.168.1.220", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
        "controller03":{"IP":"192.168.1.221", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
    },

    "compute_node":{    # 计算节点
        "compute01":{"IP":"192.168.1.237", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
        "compute02":{"IP":"192.168.1.238", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp132s0','tuncard':'enp130s0'},
        "compute03":{"IP":"192.168.1.239", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
        "compute04":{"IP":"192.168.1.240", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
        "compute05":{"IP":"192.168.1.241", "username":"root", "password":"root1234", "port":22,"mancard":'enp4s0f0','buscard':'enp8s0','tuncard':'enp130s0'},
    },

    # "other":{
    #     # "ceilometer":{"IP":"192.168.179.133", "username":"root", "password":"zcl", "port":22},
    # }
}


cd AutoMitak/core/
python entry_point.py


############################################################## 安装完成之后的后续配置 #####################################

3、cinder对接LVM存储
# 在三台控制节点上
pvcreate /dev/sdb
vgcreate cinder-volumes /dev/sdb

openstack-config --set /etc/cinder/cinder.conf lvm volume_driver  cinder.volume.drivers.lvm.LVMVolumeDriver
openstack-config --set /etc/cinder/cinder.conf lvm volume_group   cinder-volumes
openstack-config --set /etc/cinder/cinder.conf lvm iscsi_protocol iscsi
openstack-config --set /etc/cinder/cinder.conf lvm iscsi_helper   lioadm
openstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends  lvm

pcs resource restart openstack-cinder-volume


4、NFS共享存储
1）image共享
# 在NFS主机上
yum -y install nfs-utils rpcbind -y 
mkdir /opt/glance/images/ -p 

fdisk /dev/sdc <<EOF
n
p
1


w
EOF
# 格式化
mkfs.xfs /dev/sdc1

# 挂载
mount /dev/sdc1 /opt/glance/images/


echo "/opt/glance/images/ 10.0.0.0/24(rw,no_root_squash,no_all_squash,sync)" > /etc/exports 

exportfs -r 
systemctl enable rpcbind.service 
systemctl start rpcbind.service 
systemctl enable nfs-server.service 
systemctl start nfs-server.service
chmod -R 777 /opt/glance/images/


# 查看 showmount -e 10.0.0.218

# 三个控制节点挂载

mount -t nfs 10.0.0.218:/opt/glance/images/ /var/lib/glance/images/



2） nova共享
# 分区，格式化

# 分区
fdisk /dev/sdb <<EOF
n
p
1


w
EOF

# 格式化
mkfs.xfs /dev/sdb1

# 挂载
mkdir -p /opt/nova/instances/
mount /dev/sdb1 /opt/nova/instances/

# NFS共享
echo "/opt/nova/instances 10.0.0.0/24(rw,no_root_squash,no_all_squash,sync)" >> /etc/exports
exportfs -r
chmod -R 777 /opt/nova/instances/


# 5个计算节点使用NFS

showmount -e 10.0.0.218
mount -t nfs 10.0.0.218:/opt/nova/instances /var/lib/nova/instances/
chown -R nova.nova /var/lib/nova



# 上传镜像
openstack image create "cirros" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public




# 计算节点之间做nova用户的ssh互信


# 修改计算节点配置文件/etc/nova/nova.conf
image_cache_manager_interval=0 
live_migration_tunnelled=true






#######################

2018-01-23 15:04:00.040 31177 INFO nova.console.websocketproxy [req-5955bf78-e87f-4b8b-bd42-3edbd48999ca - - - - -] handler exception: The token '1a120367-a3be-41ee-8b1c-806d33323f08' is invalid or has expired


memcached做的haporoxy不好，需要修改配置，删除haproxy的memcached的11211负载均衡，修改所有配置文件为
memcached_servers = controller01:11211,controller02:11211,controller03:11211

dashboard,glance,nova,neutron,cinder

控制节点中/etc/nova/nova.conf 中default中增加
memcached_servers = controller01:11211,controller02:11211,controller03:11211




systemctl restart openstack-nova-compute neutron-openvswitch-agent.service





# 开启安全组

控制节点和计算节点修改如下配置：
vim /etc/neutron/plugins/ml2/openvswitch_agent.ini
[securitygroup]
enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver








# 负载均衡lbaasv2

1、所有控制节点
yum install openstack-neutron-lbaas
cd /etc/neutron/
#1.3编辑neutron.conf文件
service_plugins =router, neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
#1.4 编辑lbaas_agent.ini 文件
[DEFAULT]
interface_driver =neutron.agent.linux.interface.OVSInterfaceDriver
ovs_use_veth = True
device_driver = neutron_lbaas.drivers.haproxy.namespace_driver.HaproxyNSDriver
[haproxy]
user_group =haproxy
#1.5 编辑neutron_lbaas.conf文件
service_provider =LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default
#然后执行
neutron-db-manage --subproject neutron-lbaas upgrade head
systemctl restart neutron-server

# 安装neutron-lbaas-dashboard
这个是在openstack_dashboard安装的节点，一般是controller节点
git clone https://git.openstack.org/openstack/neutron-lbaas-dashboard
cd neutron-lbaas-dashboard
python setup.py install
cp neutron_lbaas_dashboard/enabled/1480project_loadbalancersv2_panel.py /usr/share/openstack-dashboard/openstack_dashboard/local/enabled/
systemctl restart httpd.service memcached.service



2、在网络节点操作如下（如果是分布式的话可以在计算节点上做如下操作）：
yum install haproxy openstack-neutron-lbaas -y

cd /etc/neutron/
#1.4 编辑neutron.conf文件
service_plugins = neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2
#1.5 编辑lbaas_agent.ini 文件
[DEFAULT]
interface_driver =neutron.agent.linux.interface.OVSInterfaceDriver
ovs_use_veth = True
device_driver = neutron_lbaas.drivers.haproxy.namespace_driver.HaproxyNSDriver
[haproxy]
user_group =haproxy
#1.6 编辑neutron_lbaas.conf文件
[service_providers]
service_provider =LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default
#1.7启动服务
systemctl start neutron-lbaasv2-agent.service



######### CMD创建负载均衡服务
neutron lbaas-loadbalancer-create --name test01 public_subnet --vip-address 192.168.240.83
neutron lbaas-listener-create --name test01 --loadbalancer test01 --protocol TCP --protocol-port 22
neutron lbaas-pool-create --name test01 --lb-algorithm ROUND_ROBIN --listener test01 --protocol TCP
neutron lbaas-member-create --subnet public_subnet --address 192.168.240.80 --protocol-port 22  test01
neutron lbaas-member-create --subnet public_subnet --address 192.168.240.86 --protocol-port 22  test01
neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type TCP --pool test01

################################ 创建另一个listener
neutron lbaas-listener-create --name test02 --loadbalancer test01 --protocol HTTP --protocol-port 80
neutron lbaas-pool-create --name test02 --lb-algorithm ROUND_ROBIN --listener test02 --protocol HTTP
neutron lbaas-member-create --subnet public_subnet --address 192.168.240.81 --protocol-port 80  test02
neutron lbaas-member-create --subnet public_subnet --address 192.168.240.82 --protocol-port 80  test02
neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type HTTP --pool test02





neutron lbaas-listener-create --name test05 --loadbalancer test01 --protocol HTTPS --protocol-port 443
neutron lbaas-pool-create --name test05 --lb-algorithm LEAST_CONNECTIONS --listener test05 --protocol HTTPS
neutron lbaas-member-create --subnet public_subnet --address 192.168.240.81 --protocol-port 443 test05
neutron lbaas-member-create --subnet public_subnet --address 192.168.240.82 --protocol-port 443 test05
neutron lbaas-healthmonitor-create --delay 5 --max-retries 2 --timeout 10 --type HTTPS --pool test05
